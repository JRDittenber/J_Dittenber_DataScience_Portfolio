---
title: "Harvard edX Data Science Capstone"
author: "***J. Dittenber***"
date: 'December 2022'
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1. ABSTRACT:

The MovieLens dataset contains over 9,000,000 observations of users' interactions with a movie rating schema over a period of time from 1995 to 2009. The variables include the userId, movieId, movie title, genres and ratings. The objective is to build a model that can predict ratings with a loss of RMSE < 0.86.RMSE is a metric that quantifies how different the predicted values are from known values when the variables are input into a model. After analysis using various models, the model with the lowest loss includes userId and  movie id and is a multiple linear regression model. 


This is a movie recommendation system created using machine learning for the capstone project of the Harvard edX Data Science Professional Certificate capstone. The R Markdown document will walk through all of the steps, code and reasoning behind each step. The outline is as follows:

1.  Abstract
2.  **Executive Summary**
3.  Importing libraries, data and creating test and train sets
4.  Exploratory data analysis and pre-processing

    1.  Data cleaning and anomaly detection

    2.  Checking that test and train data sets have correct dimensions and variables

    3.  Variable Analysis

    4.  Correlation Analysis
5.  **Methodology and Analysis**  
6.  Model creation and diagnostics
7.  **Results**
8.  **Conclusion**





\newpage 
#  2.  EXECUTIVE SUMMARY:

In this project I worked with the MovieLens dataset, a collection of ratings and metadata for a selection of movies. My goal was to build a model to predict ratings for the movies that could achieve an RMSE of less than 0.86.

To start, I sampled the data and generated 1000 models of size n=77 for a multiple linear regression model, decision tree and a random forest model. After evaluating these models RMSE on the train and test set (not the final holdout set), the evidence supported that the multiple linear regression model using userId, movieId and genres was the model with the best (lowest) RMSE. 

With this information, I built on the course methodology of utilizing the entire data set and manually calculating the parameters for the multiple linear regression model. This was necessary since the computational time and resources needed to utilize the packages in R (that can generate machine learning models) is not easily available on a laptop or PC. 

The conclusion revealed that the best model (calculated using the full 9,000,000 observations) was the model that included only the movieId and userid. 


# 3. Importing libraries, data and creating test and train sets

**Import Libraries**
```{r, results = 'hide'}
##########################################################
# Import Libraries 
##########################################################

# Note: this process could take a couple of minutes
options(repos = list(CRAN="http://cran.rstudio.com/"))
install.packages('plyr', repos ="http://cran.us.r-project.org" )
library(plyr)
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(psych)) install.packages("psych", repos = "http://cran.us.r-project.org")
if(!require(rafalib)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
if(!require(tinytex)) install.packages("tinytex", repos = "http://cran.us.r-project.org")
if(!require(formatR)) install.packages("formatR", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")



library(tidyr)
library(dslabs)
library(tidyverse)
library(caret)
library(data.table)
library(psych)
library(rafalib)
library(tinytex)
library(formatR)
library(rpart)
library(ggplot2)
library(nnet)
library(randomForest)
library(dplyr)
library(glmnet)


```

```{r, echo=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later

test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```


# 4. Exploratory Data Analysis & Preprocessing



## **We will examine the variables and their types:**
```{r,   tidy = TRUE}
vars <- tibble(name = names(edx), class = class(edx))
vars


```

**Change the timestamp to date and year of rating**
```{r Deal With timestamp, keep_tex = TRUE}
new_edx <- edx %>%  mutate(date_rated = as.POSIXct(timestamp, origin = "1970-01-01", tz="UTC"),
                         year_rated = format(date_rated, "%Y"))
```

**Verify that the dates are converted correctly and a new column is formed**
```{r Check Mutated Data Frame, tidy =1}


head(new_edx)

#check the data types of the mutated data set

vars <- tibble(name = names(new_edx), class = class(new_edx))
vars


#check the columns for date_rated and year_rated 

min(new_edx$date_rate); min(new_edx$year_rate); max(new_edx$date_rate); max(new_edx$year_rate)

#don't need timestamp now 

new_edx$timestamp <- NULL


```

**Check for NA/Nulls by comparing lengths after applying na.omit()** 
```{r,    tidy = TRUE}

#create new dataframe after applying na.omit -this will remove observations with NA
check_df <- na.omit(new_edx)
length(check_df$rating); length(new_edx$rating)

rm(check_df)

```

## Variable Analysis 

I will examine each variable for informative characteristics.

**UserId** 

```{r}
#length of the variables 

length(new_edx$userId)

#unique users 

n_distinct(new_edx$userId)

```

\newpage

```{r, tidy =1}
#users sorted by rating count 

new_edx %>%  group_by(userId) %>% count() %>% arrange(desc(n))

#check for any ratings of 0

new_edx %>%  group_by(userId) %>% count() %>%  arrange(n)

#examine the distribution of movies rated per user 

new_edx %>% 
    dplyr::count(userId) %>% 
    ggplot(aes(n)) + 
    geom_histogram(bins = 30, color = 'black') + 
    scale_x_log10() +
    ggtitle("Distribution of Users") +
    xlab("Number of Movies Rated") +
    ylab("Number of Users")  +
    theme_minimal()



```
\newpage 

**Ratings**

```{r}
#count  number of ratings for each rating
rating_counts <- new_edx %>%
  group_by(rating) %>%
  summarise(count = n())

#bar chart
ggplot(rating_counts, aes(x = rating, y = count)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = count), vjust = -0.2, size = 3.5) +
  ggtitle("Distribution of Ratings") +
  xlab("Rating Scale: 0 to 5 with increments of 0.5") +
  ylab("Number of Ratings") +
  theme_minimal()


#count of each rating level 
rating_counts %>% knitr::kable()





# boxplot for ratings per year

ratings_by_year <- new_edx %>%
  group_by(year_rated, movieId) %>%
  summarise(num_ratings = n()) %>%
  mutate(sqrt_num_ratings = sqrt(num_ratings))

# create the plot
ggplot(ratings_by_year, aes(x = year_rated, y = sqrt_num_ratings)) +
  geom_boxplot() +
  ggtitle("Number of Ratings for Each Movie by Year") +
  xlab("Year") +
  ylab("Number of Ratings (sqrt transformed)") +
  theme_minimal()


#over time per week 
ratings_by_week <- new_edx %>%
  mutate(week = lubridate::round_date(date_rated, "week")) %>%
  group_by(week) %>%
  summarise(mean_rating = mean(rating))

#create the plot
ggplot(ratings_by_week, aes(x = week, y = mean_rating)) +
  geom_line() +
  geom_smooth(method = 'loess')+
  ggtitle("Average Rating by Week") +
  xlab("Week") +
  ylab("Average Rating") +
  theme_minimal()





```

**Genres** 
```{r,  tidy = TRUE}
#examine genres as grouped 

#total length
length(new_edx$genres)

#distinct genres combinations
n_distinct(new_edx$genres)

#sort by rating count 
new_edx %>% 
        group_by(genres) %>%
        count()          %>%
        arrange(desc(n))

#sort by highest average rating (note that rating is categorical, yet we can still apply this)
new_edx %>% group_by(genres) %>% summarize(mean_rating = mean(rating),                                      sd_rating= sd(rating)) %>%       
                            arrange(desc(mean_rating)) 

#sort by lowest average rating (note that rating is categorical, yet we can still apply this)
new_edx %>%  group_by(genres) %>% summarize(mean_rating = mean(rating),                                       sd_rating= sd(rating)) %>%                                                      arrange(mean_rating)
        

#separate the combinations of genres
genres_edx <- new_edx %>% separate_rows(genres, sep = "\\|")
head(genres_edx)

str(genres_edx)

#determine how many observations 
length(genres_edx$genres)


#distinct genres when split 
n_distinct(genres_edx$genres)


#sort by highest average rating (note that rating is categorical, yet we can still apply this)
genres_edx %>% group_by(genres) %>% summarize(mean_rating = mean(rating),                                       sd_rating= sd(rating),                                                         count = n()) %>% arrange(desc(mean_rating)) 


#sort by highest count
genres_edx %>% group_by(genres) %>% summarize(mean_rating = mean(rating),                                       sd_rating= sd(rating),                                                          count = n())%>% arrange(desc(count)) 

#sort by lowest average rating (note that rating is categorical, yet we can still apply this)
genres_edx %>% group_by(genres) %>% summarize(mean_rating = mean(rating), sd_rating= sd(rating)) %>%   arrange(mean_rating) 


#filter the movies with at least 50 ratings
movies_50 <- new_edx %>%
  group_by(movieId) %>%
  filter(n() >= 50) %>%
  ungroup()


#count the number of ratings for each genre
genre_counts <- movies_50 %>%
  group_by(genres) %>%
  summarise(count = n()) %>%
  arrange(desc(count))


#calculate the average rating for each genre
genre_ratings <- new_edx %>%
  group_by(genres) %>%
  summarise(average_rating = mean(rating)) %>%
  arrange(desc(average_rating)) %>%
  head(3)

#bar chart
ggplot(genre_ratings, aes(x = genres, y = average_rating)) +
  geom_col() +
  geom_text(aes(label = round(average_rating, 2)), vjust = -0.2, size = 3.5) +
  ggtitle("Top 3 Genres by Average Rating") +
  xlab("Genres") +
  ylab("Average Rating") +
  theme_minimal()


#Separate the Genres 

#calculate the average rating for each genre
genre_ratings <- genres_edx %>%
  group_by(genres) %>%
  summarise(mean_rating = mean(rating), sd_rating= sd(rating), count = n()) %>%
  arrange(desc(mean_rating)) %>%
  head(3)

#create bar chart
ggplot(genre_ratings, aes(x = genres, y = mean_rating)) +
  geom_col() +
  geom_text(aes(label = round(mean_rating, 2)), vjust = -0.2, size = 3.5) +
  ggtitle("Top 3 Indiviudal Genres by Average Rating") +
  xlab("Genre") +
  ylab("Average Rating") +
  theme_minimal()

```



# 5.  METHODS AND ANALYSIS 

## Methodology for the Initial Analysis

Given that the data contains over 9 million observations it seems prudent to sample from these 9 million observations and get an idea of a baseline or even to create a final model.

Models that use the entire data set can have problems in deployment and implementation. Overly complicated models, while reaping low performance metrics have the disadvantage of being impractical to use and taking too much time and computational power.

Therefore, the first attempt at generating a model will be done by sampling the the data and making inferences about the population (of 9 million observations). Afterward, the methodologies utilized in the course will be implemented and expanded. 

An a priori power analysis was conducted and it was determined that a sample size of
n=77 was sufficient for an alpha = 0.05 and a beta = 0.8. Note that in general, 
prior research indicates that a rule of them is that there must be 10 observations per 
variable to reach sufficient statistical power. 


The steps: 

1. Take 1000 samples of size n = 77
2. For each of these samples build a linear regression model 
3. For each of the linear regression models calculate the RMSE
4. Store all of the RMSE in a list
5. Report the minimum RMSE 
6. Repeat and add the additional predictors

Furthermore, the approach will be implemented on 

1. Linear Regression 
2. Decision Tree 
3. Random Forest 


Summary: In each case, the model with the lowest RMSE is the model that uses all of the 
predictors, with the exception of the time related predictors (which are excluded). It is worth noting that both the movieId and Title (of the movie) are likely contributing the same amount of variation, and thus, according to the analysis, the model with the lowest RMSE will contain the user Id, movie Id and genre. 

Additionally, there is not evidence of overfitting between the test and training sets as they have roughly the same RMSE.

The best model was the multiple linear regression model that used user id, movie id and genres as predictors. Note that these predictors were converted to factors, yet we can still achieve a prediction with an RMSE within the boundaries set forth.


# Train and Test Split
```{r}

set.seed(42)

test_index <- createDataPartition(y = edx$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")





```

# 6. MODEL CREATION AND DIAGNOSTICS

 

# Sampling Approach - Mutli-Linear Regression Model 
```{r, tidy = TRUE}


#sampling for baseline RMSE 
rmse_calc <- function(df, formula) {
  model <- lm(formula, data = df)
  sqrt(mean((predict(model) - df$rating)^2))
}

# sample data 1000 times and calculate RMSE for each model
rmse_samples <- data.frame(method = c("User Id Only", "User Id and Movie Id ", "User Id, Movie Id, Genres ", "Userd Id, Movie Id, Genres, Title"), 
                           RMSE = sapply(1:4, function(i) {
                             formulas <- c("rating ~ userId", "rating ~ userId + movieId", "rating ~ userId + movieId + genres", "rating ~ userId + movieId + genres + title")
                             mean(replicate(1000, rmse_calc(sample_n(train_set,77, replace = TRUE), formulas[i])))
                           }))

#print RMSE values
rmse_samples %>%  knitr::kable()



```
# Run on the test set 
```{r,  tidy =  TRUE}

#sample data 1000 times and calculate RMSE for each model
rmse_samples <- data.frame(method = c("User Id Only", "User Id and Movie Id ", "User Id, Movie Id, Genres ", "Userd Id, Movie Id, Genres, Title"), 
                           RMSE = sapply(1:4, function(i) {
                             formulas <- c("rating ~ userId", "rating ~ userId + movieId", "rating ~ userId + movieId + genres", "rating ~ userId + movieId + genres + title")
                             # Sample the data 1000 times, calculate the RMSE for each sample, and return the mean RMSE value
                             mean(replicate(1000, rmse_calc(sample_n(test_set,77, replace =TRUE), formulas[i])))
                           }))

#print RMSE values
knitr::kable(rmse_samples)


```

# Sampling Approach - Decision Tree
```{r, tidy  = TRUE}

#sample data 1000 times and calculate RMSE for each model
rmse_samples <- data.frame(method = c("User Id Only", "User Id and Movie Id ", "User Id, Movie Id, Genres ", "Userd Id, Movie Id, Genres, Title"), 
                           RMSE = sapply(1:4, function(i) {
                             formulas <- c("rating ~ userId", "rating ~ userId + movieId", "rating ~ userId + movieId + genres", "rating ~ userId + movieId + genres + title")
                             mean(replicate(1000, rmse_calc(sample_n(train_set, 77, replace = TRUE), formulas[i])))
                           }))

#print RMSE values
rmse_samples |> knitr::kable()



```

# Run on test set 

```{r,  tidy  = TRUE}

#sample data 1000 times and calculate RMSE for each model
rmse_samples <- data.frame(method = c("User Id Only", "User Id and Movie Id ", "User Id, Movie Id, Genres ", "Userd Id, Movie Id, Genres, Title"), 
                           RMSE = sapply(1:4, function(i) {
                             formulas <- c("rating ~ userId", "rating ~ userId + movieId", "rating ~ userId + movieId + genres", "rating ~ userId + movieId + genres + title")
                             mean(replicate(1000, rmse_calc(sample_n(test_set, 77, replace = TRUE), formulas[i])))
                           }))

#Print RMSE values
rmse_samples %>% knitr::kable()


```

# Sampling Approach - Random Forest 
```{r,  tidy  = TRUE}

#sample data 1000 times and calculate RMSE for each model
rmse_samples <- data.frame(method = c("User Id Only", "User Id and Movie Id ", "User Id, Movie Id, Genres ", "Userd Id, Movie Id, Genres, Title"), 
                           RMSE = sapply(1:4, function(i) {
                             formulas <- c("rating ~ userId", "rating ~ userId + movieId",  "rating ~ userId + movieId + genres", "rating ~ userId + movieId + genres + title")
                             mean(replicate(1000, rmse_calc(sample_n(train_set, 77,  replace=TRUE), as.formula(formulas[i]))))
                           }))

#print RMSE values
rmse_samples |> knitr::kable()

```

# Run on test set
```{r,  tidy  = TRUE}
#sample data 1000 times and calculate RMSE for each model
rmse_samples <- data.frame(method = c("User Id Only", "User Id and Movie Id ", "User Id, Movie Id, Genres ", "Userd Id, Movie Id, Genres, Title"), 
                           RMSE = sapply(1:4, function(i) {
                             formulas <- c("rating ~ userId", "rating ~ userId + movieId",  "rating ~ userId + movieId + genres", "rating ~ userId + movieId + genres + title")
                             mean(replicate(1000, rmse_calc(sample_n(test_set, 77,  replace=TRUE), as.formula(formulas[i]))))
                           }))

#print RMSE values
rmse_samples |> knitr::kable()

```

# Expanding on the Model from the Course:

The model from the course used the following method. Note that here, we also have that the predictors are not treated as factors. We will expand on this idea and incorporate the results of the previous analysis.

```{r,  tidy  = TRUE}


rm(list = setdiff(ls(), c('edx', 'final_holdout_test')))

set.seed(42)

test_index <- createDataPartition(y = edx$rating, times = 1,
                                  p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]

test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")





mu_hat <- mean(train_set$rating)
mu_hat

naive_rmse <- RMSE(test_set$rating, mu_hat)
naive_rmse

predictions <- rep(2.5, nrow(test_set))
RMSE(test_set$rating, predictions)

rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)



# fit <- lm(rating ~ as.factor(userId), data = movielens)

mu <- mean(train_set$rating) 
movie_avgs <- train_set %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))


predicted_ratings <- mu + test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     .$b_i



model_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))

rmse_results %>% knitr::kable()



# lm(rating ~ as.factor(movieId) + as.factor(userId))

user_avgs <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))


predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     .$pred

model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()



# lm(rating ~ as.factor(movieId) + as.factor(userId)) + genres



genre_avgs <- test_set %>% 
     left_join(user_avgs, by='userId') %>%
     group_by(genres) %>%
     summarize(b_g = mean(rating - mu - b_u)) 


predicted_ratings <- test_set %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     left_join(genre_avgs, by = 'genres') %>%
     mutate(pred = mu + b_u + b_g) %>%
     .$pred


model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model + Genres",  
                                     RMSE = model_3_rmse ))


rmse_results %>% knitr::kable() 

```
\newpage 
# 7. RESULTS 
The final model for the purposes of this analysis is the model that includes the userId and movie effects and that adding genres increases the RMSE. Next, we run the model on the final holdout set. According the RMSE, the model generalizes well to new data and there is no evidence of overfitting for this particular set of predictors.





 
#  **Sample Size v. Accuracy** 

Upon applying the model to the final hold out set, and in accordance with this study and constrained by the limitations herein, the best model is the multiple linear regression model using the aforementioned predictors.

The MovieLens dataset is extremely large. In fact, as the sample size increases, the accuracy decreases. As shown below: 

## **Demonstrate Relationship Between Sample Size and Accuracy **
```{r,  keep_tex = TRUE, echo=FALSE}

mse <- function(predictions, actual) {
  # Calculate the squared error for each prediction
  squared_error <- (predictions - actual)^2
  
  # Calculate the mean squared error
  mean(squared_error)
}




new_edx <- edx %>%  mutate(date_rated = as.POSIXct(timestamp, origin = "1970-01-01", tz="UTC"),
                         year_rated = format(date_rated, "%Y"))


data <- new_edx


data <- data[, -c(6, 7)]


set.seed(123)  # set the random seed for reproducibility
train_indices <- createDataPartition(data[, 3], p = 0.8, list = FALSE)
train <- data[train_indices, ]
test <- data[-train_indices, ]


iterations <- 10


accuracy <- numeric(iterations)

#loop over iterations
for (i in 1:iterations) {
  n <- 100

  sample_indices <- sample(1:nrow(train), size = n)
  sample_train <- train[sample_indices, ]
  
  #train a model
  model <- randomForest(x = sample_train[, -3], y = sample_train[, 3])
  

  predictions <- predict(model, test)
  
  
  accuracy[i] <- mse(predictions, test[, 3])
  
  
  n <- n + 100
}


sample_sizes <- seq(100, 100*iterations, by = 100)


plot(accuracy ~ sample_sizes, type = "l")


```

As demonstrated, the accuracy appears to converge as the sample size increases. This seems to indicate that having 9,000,000 plus observations does not make the predictive power greater than a model with fewer samples from which it is built. However, it is difficult to test this using conventional methods and conventional computing power.  

\newpage 

# 9. **CONCLUSION**

The results of the analysis indicate that of the models tested in this analysis, a multiple linear regression utilizing as predictors, user id and movie id provide the best RMSE. Below, we will demonstrate by running the modeling procedure on the final hold out data set. The sampling method seems to be contradicted by the results obtained by utilizing the full data set. That is, that utilizing the genres predictor leads to lower and more stable RMSE for both test and the train set. This contradiction arises in the the fact that the RMSE increases when genres is included in the final model using the method that utilizes the entire data set. Ideas for future studies might include the ability to use more computational power to be able to run case-wise diagnostics and utilize modeling packages that can generate reports about the statistical significance of the variables and the ability to deal with extremely large categorical variables with large numbers of levels. Ideally, the study would seek to identify the ideal (minimal and optimal) number of observations and predictors. 

```{r} 
#note - I used less white space so that the results would be on page 
rm(list = setdiff(ls(), c('final_holdout_test')))
set.seed(42)
mu_hat <- mean(final_holdout_test$rating)
mu_hat
naive_rmse <- RMSE(final_holdout_test$rating, mu_hat)
naive_rmse
predictions <- rep(2.5, nrow(final_holdout_test))
RMSE(final_holdout_test$rating, predictions)
rmse_results <- data_frame(method = "Just the average", RMSE = naive_rmse)
# fit <- lm(rating ~ as.factor(userId), data = movielens)

mu <- mean(final_holdout_test$rating) 
movie_avgs <- final_holdout_test %>% 
     group_by(movieId) %>% 
     summarize(b_i = mean(rating - mu))
predicted_ratings <- mu + final_holdout_test %>% 
     left_join(movie_avgs, by='movieId') %>%
     .$b_i
model_1_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie Effect Model",
                                     RMSE = model_1_rmse ))
# lm(rating ~ as.factor(movieId) + as.factor(userId))
user_avgs <- final_holdout_test %>% 
     left_join(movie_avgs, by='movieId') %>%
     group_by(userId) %>%
     summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- final_holdout_test %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     mutate(pred = mu + b_i + b_u) %>%
     .$pred
model_2_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model",  
                                     RMSE = model_2_rmse ))
# lm(rating ~ as.factor(movieId) + as.factor(userId)) + genres
genre_avgs <- final_holdout_test %>% 
     left_join(user_avgs, by='userId') %>%
     group_by(genres) %>%
     summarize(b_g = mean(rating - mu - b_u)) 


predicted_ratings <- final_holdout_test %>% 
     left_join(movie_avgs, by='movieId') %>%
     left_join(user_avgs, by='userId') %>%
     left_join(genre_avgs, by = 'genres') %>%
     mutate(pred = mu + b_u + b_g) %>%
     .$pred


model_3_rmse <- RMSE(predicted_ratings, final_holdout_test$rating)
rmse_results <- bind_rows(rmse_results,
                          data_frame(method="Movie + User Effects Model + Genres",  
                                     RMSE = model_3_rmse ))


rmse_results %>% knitr::kable()






```